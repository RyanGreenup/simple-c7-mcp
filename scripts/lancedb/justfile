package_name := `python -c "import tomllib; print(tomllib.load(open('pyproject.toml', 'rb'))['project']['name'])"`

# Show available commands
default:
    @just --list

# Run the CLI without installing
run *args:
    uv run lance-db-example {{ args }}

lint:
    ruff check
    uv run -- pyright .
    uv run -- basedpyright .

format:
    ruff check --fix
    ruff format

test:
    uv run pytest -v .



# Install the CLI tool globally
install:
    uv tool install . --force

# Uninstall the CLI tool
uninstall:
    uv tool uninstall {{ package_name }}

# Reinstall the CLI tool
reinstall:
    uv tool install . --force --reinstall

# LanceDB specific commands

# Ingest the sample document
ingest-sample:
    uv run lance-db-example ingest sample_document.txt

# Ingest a custom file
ingest file:
    uv run lance-db-example ingest {{file}}

# Search for a query
search query:
    uv run lance-db-example search "{{query}}"

# Search with limit
search-limit query limit="10":
    uv run lance-db-example search "{{query}}" --limit {{limit}}

# List all tables in the database
list-tables:
    uv run lance-db-example list-tables

# Show statistics for a table
stats:
    uv run lance-db-example stats

# Complete demo workflow
demo:
    @echo "üöÄ Running complete demo..."
    @echo ""
    @echo "Step 1: Ingesting sample document"
    just ingest-sample
    @echo ""
    @echo "Step 2: Searching for 'neural networks'"
    just search "neural networks"
    @echo ""
    @echo "Step 3: Searching for 'supervised learning'"
    just search "supervised learning"
    @echo ""
    @echo "Step 4: Searching for 'healthcare applications'"
    just search "healthcare applications"
    @echo ""
    @echo "Step 5: Listing tables"
    just list-tables
    @echo ""
    @echo "Step 6: Showing statistics"
    just stats
    @echo ""
    @echo "‚úÖ Demo complete!"

# Clean the database
clean-db:
    rm -rf lancedb_data
    @echo "‚úÖ Database cleaned"

# Delete documents by library name
delete-library library db_path="./lancedb_data" table="documents":
    #!/usr/bin/env python3
    import sys
    import lancedb
    from pathlib import Path

    library = "{{ library }}"
    db_path = Path("{{ db_path }}")
    table_name = "{{ table }}"

    if not db_path.exists():
        print(f"‚ùå Database not found at: {db_path}")
        sys.exit(1)

    print(f"üóëÔ∏è  Deleting documents with library='{library}'...")

    try:
        db = lancedb.connect(db_path)
        table = db.open_table(table_name)

        # Count before
        import pyarrow.compute as pc
        before = len(table.to_pandas())
        to_delete = len(table.to_pandas()[table.to_pandas()['library'] == library])

        if to_delete == 0:
            print(f"‚ö†Ô∏è  No documents found with library='{library}'")
            sys.exit(0)

        # Delete using LanceDB API
        table.delete(f"library = '{library}'")

        # Compact to reclaim space
        table.compact_files()

        print(f"‚úÖ Deleted {to_delete} documents")
        print(f"   Before: {before} docs, After: {before - to_delete} docs")

    except Exception as e:
        print(f"‚ùå Error: {e}")
        sys.exit(1)

# Show CLI help
help:
    uv run lance-db-example --help

# Open a DuckDB SQL shell with Lance data loaded
shell db_path="./lancedb_data" table="documents":
    #!/usr/bin/env python3
    import sys
    import subprocess
    from pathlib import Path

    db_path = "{{ db_path }}"
    table = "{{ table }}"

    if not Path(db_path).exists():
        print(f"‚ö†Ô∏è  Database not found at: {db_path}")
        print("Run 'just ingest-sample' first to create a database")
        sys.exit(1)

    print("ü¶Ü Opening DuckDB SQL shell with Lance data...")
    print(f"üìä Database: {db_path}")
    print(f"üìã Table: {table}")
    print()
    print("Example queries:")
    print("  SELECT COUNT(*) FROM docs;")
    print("  SELECT text, chunk_index FROM docs LIMIT 5;")
    print("  SELECT * FROM docs WHERE chunk_index < 3;")
    print("  .schema docs")
    print()

    # Open DuckDB with init commands using process substitution (no temp file)
    view_path = f"{db_path}/{table}.lance"

    # Build init commands as separate lines to avoid justfile parsing issues
    commands = []
    commands.append("INSTALL lance FROM community;")
    commands.append("LOAD lance;")
    commands.append(f"CREATE OR REPLACE VIEW docs AS SELECT * FROM __lance_scan('{view_path}');")
    commands.append(".timer on")
    commands.append("SELECT 'üéØ Connected! Table \"docs\" has ' || COUNT(*)::VARCHAR || ' rows' as status FROM docs;")
    init_script = "\n".join(commands)

    # Build the bash command with heredoc
    bash_cmd = f"duckdb -init <(cat <<'INITEOF'\n{init_script}\nINITEOF\n)"

    subprocess.run(['bash', '-c', bash_cmd])

# Fetch library docs from Context7 and ingest into LanceDB
fetch-library library query="":
    #!/usr/bin/env python3
    import json
    import sys
    import os
    import subprocess
    from pathlib import Path
    import urllib.request

    library = "{{ library }}"
    query = "{{ query }}" or f"Documentation for {library}"

    print(f"üîç Resolving library: {library}")

    # Step 1: Call Context7 MCP API directly with proper JSON parsing
    mcp_url = "https://mcp.context7.com/mcp"

    # Get API key from environment if available
    api_key = os.environ.get('CONTEXT7_API_KEY', '')

    # Construct MCP request
    request_data = {
        "jsonrpc": "2.0",
        "id": 1,
        "method": "tools/call",
        "params": {
            "name": "resolve-library-id",
            "arguments": {
                "libraryName": library,
                "query": query
            }
        }
    }

    # Make the API request
    headers = {
        'Content-Type': 'application/json',
        'Accept': 'application/json, text/event-stream'
    }
    if api_key:
        headers['CONTEXT7_API_KEY'] = api_key

    req = urllib.request.Request(
        mcp_url,
        data=json.dumps(request_data).encode('utf-8'),
        headers=headers
    )

    try:
        with urllib.request.urlopen(req) as response:
            result = json.loads(response.read().decode('utf-8'))
    except Exception as e:
        print(f"‚ùå API request failed: {e}")
        sys.exit(1)

    # Parse the JSON response properly
    try:
        # Extract the text content from the MCP response
        content_text = result['result']['content'][0]['text']

        # Parse all library options from the text
        libraries = []
        current_lib = {}

        for line in content_text.split('\n'):
            line = line.strip()

            if line.startswith('- Title:'):
                # Save previous library if exists
                if current_lib.get('id'):
                    libraries.append(current_lib)
                current_lib = {'title': line.split('- Title:')[1].strip()}

            elif line.startswith('- Context7-compatible library ID:'):
                current_lib['id'] = line.split('Context7-compatible library ID:')[1].strip()

            elif line.startswith('- Description:'):
                current_lib['description'] = line.split('- Description:')[1].strip()

            elif line.startswith('- Benchmark Score:'):
                try:
                    current_lib['score'] = float(line.split('- Benchmark Score:')[1].strip())
                except:
                    current_lib['score'] = 0

        # Save the last library
        if current_lib.get('id'):
            libraries.append(current_lib)

        if not libraries:
            print("‚ùå No libraries found in response")
            print("\nAPI Response:")
            print(json.dumps(result, indent=2))
            sys.exit(1)

        # Sort by benchmark score (highest first)
        libraries.sort(key=lambda x: x.get('score', 0), reverse=True)

        # Pick the best match (highest score)
        best_match = libraries[0]
        library_id = best_match['id']

        print(f"‚úì Selected: {best_match['title']} (score: {best_match.get('score', 'N/A')})")
        if len(libraries) > 1:
            print(f"  (Found {len(libraries)} options, selected highest score)")

    except (KeyError, IndexError, TypeError) as e:
        print(f"‚ùå Failed to parse API response: {e}")
        print("\nRaw response:")
        print(json.dumps(result, indent=2))
        sys.exit(1)

    print(f"‚úì Resolved library_id: {library_id}")

    # Step 2: Construct URL and download docs
    # library_id format: /websites/daisyui or /github/org/repo
    url = f"https://context7.com{library_id}/llms.txt"
    print(f"üì• Downloading from: {url}")

    try:
        with urllib.request.urlopen(url) as response:
            content = response.read().decode('utf-8')
    except Exception as e:
        print(f"‚ùå Failed to download: {e}")
        sys.exit(1)

    # Step 3: Save to tests/data/{library}.md
    safe_name = library.lower().replace(' ', '-').replace('.', '-')
    output_file = Path(f"tests/data/{safe_name}.md")
    output_file.parent.mkdir(parents=True, exist_ok=True)

    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(content)

    print(f"‚úì Saved to: {output_file}")
    print(f"üìä Size: {len(content)} bytes")

    # Step 4: Ingest into LanceDB
    print(f"\nüìö Ingesting into LanceDB...")
    subprocess.run([
        'uv', 'run', 'lance-db-example', 'ingest',
        str(output_file),
        '--library', safe_name
    ])

    print(f"\n‚úÖ Complete! Library '{library}' is now in LanceDB")
    print(f"Try: just search '{library}' --limit 5")

# Demo: Ingest solid.md and search for "throw redirect" (uses markdown-h3 by default)
demo-solid:
    @echo "üìö Ingesting solid.md with markdown h3 chunking..."
    uv run lance-db-example ingest tests/data/solid.md --library solid-js
    @echo ""
    @echo "üîç Searching for 'throw redirect'..."
    uv run lance-db-example search "throw redirect" --limit 3
    @echo ""
    @echo "‚úÖ Demo complete! Check the chunking above."

# Demo: Ingest with character-based chunking (old behavior)
demo-solid-char:
    @echo "üìö Ingesting solid.md with character-based chunking..."
    uv run lance-db-example ingest tests/data/solid.md --chunking-strategy character --chunk-size 1000
    @echo ""
    @echo "üîç Searching for 'throw redirect'..."
    uv run lance-db-example search "throw redirect" --limit 3
    @echo ""
    @echo "‚úÖ Demo complete!"

# Demo: Fetch and ingest daisyUI documentation
demo-daisyui:
    just fetch-library "daisyUI"
    @echo ""
    @echo "üîç Searching daisyUI docs..."
    uv run lance-db-example search "button component" --limit 3

# Benchmark CPU vs GPU search speed
benchmark-cpu query="button component" limit="5":
    #!/usr/bin/env bash
    set -e

    echo "üî¨ Benchmarking search performance..."
    echo "Query: {{ query }}"
    echo "Limit: {{ limit }}"
    echo ""

    # Test GPU (if available)
    echo "‚ö° Testing with GPU..."
    START=$(date +%s.%N)
    uv run lance-db-example search "{{ query }}" --limit {{ limit }} > /dev/null 2>&1
    END=$(date +%s.%N)
    GPU_TIME=$(echo "$END - $START" | bc)
    echo "GPU time: ${GPU_TIME}s"
    echo ""

    # Test CPU (force by hiding GPU)
    echo "üêå Testing with CPU (hiding GPU with CUDA_VISIBLE_DEVICES='')..."
    START=$(date +%s.%N)
    CUDA_VISIBLE_DEVICES='' uv run lance-db-example search "{{ query }}" --limit {{ limit }} > /dev/null 2>&1
    END=$(date +%s.%N)
    CPU_TIME=$(echo "$END - $START" | bc)
    echo "CPU time: ${CPU_TIME}s"
    echo ""

    # Calculate speedup
    SPEEDUP=$(echo "scale=2; $CPU_TIME / $GPU_TIME" | bc)
    echo "üìä Results:"
    echo "  GPU: ${GPU_TIME}s"
    echo "  CPU: ${CPU_TIME}s"
    echo "  Speedup: ${SPEEDUP}x faster on GPU"

# Quick CPU benchmark (single run)
bench-cpu:
    just benchmark-cpu "search query" 3

# Time a search on CPU only (with output)
search-cpu query="button component" limit="5":
    @echo "üêå Running search on CPU (this may take a while)..."
    @echo "Query: {{ query }}"
    @echo ""
    time CUDA_VISIBLE_DEVICES='' uv run lance-db-example search "{{ query }}" --limit {{ limit }}

# Benchmark multiple queries (shows real GPU benefit after model loading)
benchmark-multi:
    #!/usr/bin/env bash
    set -e

    echo "üî¨ Multi-query benchmark (3 queries, model stays loaded)..."
    echo ""

    QUERIES=("button component" "authentication" "API routing")

    # GPU benchmark
    echo "‚ö° GPU (3 sequential queries)..."
    START=$(date +%s.%N)
    for q in "${QUERIES[@]}"; do
        uv run lance-db-example search "$q" --limit 3 > /dev/null 2>&1
    done
    END=$(date +%s.%N)
    GPU_TOTAL=$(echo "$END - $START" | bc)
    GPU_AVG=$(echo "scale=3; $GPU_TOTAL / 3" | bc)
    echo "Total: ${GPU_TOTAL}s (avg: ${GPU_AVG}s per query)"
    echo ""

    # CPU benchmark
    echo "üêå CPU (3 sequential queries)..."
    START=$(date +%s.%N)
    for q in "${QUERIES[@]}"; do
        CUDA_VISIBLE_DEVICES='' uv run lance-db-example search "$q" --limit 3 > /dev/null 2>&1
    done
    END=$(date +%s.%N)
    CPU_TOTAL=$(echo "$END - $START" | bc)
    CPU_AVG=$(echo "scale=3; $CPU_TOTAL / 3" | bc)
    echo "Total: ${CPU_TOTAL}s (avg: ${CPU_AVG}s per query)"
    echo ""

    # Results
    SPEEDUP=$(echo "scale=2; $CPU_TOTAL / $GPU_TOTAL" | bc)
    echo "üìä Results:"
    echo "  GPU total: ${GPU_TOTAL}s (${GPU_AVG}s avg)"
    echo "  CPU total: ${CPU_TOTAL}s (${CPU_AVG}s avg)"
    echo "  Speedup: ${SPEEDUP}x faster on GPU"
    echo ""
    echo "Note: First query includes model loading time (~15-20s)"
    echo "      Subsequent queries show true inference speed"

# Benchmark with Qwen3-Embedding-4B model
benchmark-4b:
    #!/usr/bin/env bash
    set -e

    MODEL="Qwen/Qwen3-Embedding-4B"
    DB="lancedb_data_4b"

    # Check if 4B database exists
    if [ ! -d "$DB" ]; then
        echo "‚ö†Ô∏è  4B model database not found at $DB"
        echo "Create it first:"
        echo "  just ingest-4b tests/data/solid.md"
        exit 1
    fi

    echo "üî¨ Benchmarking Qwen3-Embedding-4B (8GB VRAM)..."
    echo "Using database: $DB"
    echo ""

    QUERIES=("button component" "authentication" "API routing")

    # GPU benchmark
    echo "‚ö° GPU with 4B model..."
    START=$(date +%s.%N)
    for q in "${QUERIES[@]}"; do
        uv run lance-db-example search "$q" --limit 3 --model "$MODEL" --db "$DB" > /dev/null 2>&1
    done
    END=$(date +%s.%N)
    GPU_TOTAL=$(echo "$END - $START" | bc)
    GPU_AVG=$(echo "scale=3; $GPU_TOTAL / 3" | bc)
    echo "Total: ${GPU_TOTAL}s (avg: ${GPU_AVG}s per query)"
    echo ""

    # CPU benchmark
    echo "üêå CPU with 4B model..."
    START=$(date +%s.%N)
    for q in "${QUERIES[@]}"; do
        CUDA_VISIBLE_DEVICES='' uv run lance-db-example search "$q" --limit 3 --model "$MODEL" --db "$DB" > /dev/null 2>&1
    done
    END=$(date +%s.%N)
    CPU_TOTAL=$(echo "$END - $START" | bc)
    CPU_AVG=$(echo "scale=3; $CPU_TOTAL / 3" | bc)
    echo "Total: ${CPU_TOTAL}s (avg: ${CPU_AVG}s per query)"
    echo ""

    SPEEDUP=$(echo "scale=2; $CPU_TOTAL / $GPU_TOTAL" | bc)
    echo "üìä Results (Qwen3-4B):"
    echo "  GPU: ${GPU_TOTAL}s (${GPU_AVG}s avg)"
    echo "  CPU: ${CPU_TOTAL}s (${CPU_AVG}s avg)"
    echo "  Speedup: ${SPEEDUP}x"
    echo "  VRAM: ~8GB"
    echo "  Embedding dim: 2560"

# Compare 8B vs 4B models on GPU (model loading + inference)
compare-models:
    #!/usr/bin/env bash
    set -e

    echo "üî¨ Comparing Qwen3-8B vs Qwen3-4B model loading..."
    echo "Note: Different models need separate databases due to different embedding dims"
    echo ""

    # Check databases exist
    if [ ! -d "lancedb_data" ]; then
        echo "‚ö†Ô∏è  8B database not found. Run: just ingest-sample"
        exit 1
    fi
    if [ ! -d "lancedb_data_4b" ]; then
        echo "‚ö†Ô∏è  4B database not found. Creating it..."
        just ingest-4b tests/data/solid.md
    fi

    QUERY="button component"

    # Test 8B
    echo "‚ö° Testing Qwen3-Embedding-8B (16GB VRAM, dim=4096)..."
    START=$(date +%s.%N)
    for i in {1..3}; do
        uv run lance-db-example search "$QUERY" --limit 3 --model "Qwen/Qwen3-Embedding-8B" --db lancedb_data > /dev/null 2>&1
    done
    END=$(date +%s.%N)
    TIME_8B=$(echo "$END - $START" | bc)
    AVG_8B=$(echo "scale=3; $TIME_8B / 3" | bc)
    echo "Total: ${TIME_8B}s (avg: ${AVG_8B}s per query)"
    echo ""

    # Test 4B
    echo "‚ö° Testing Qwen3-Embedding-4B (8GB VRAM, dim=2560)..."
    START=$(date +%s.%N)
    for i in {1..3}; do
        uv run lance-db-example search "$QUERY" --limit 3 --model "Qwen/Qwen3-Embedding-4B" --db lancedb_data_4b > /dev/null 2>&1
    done
    END=$(date +%s.%N)
    TIME_4B=$(echo "$END - $START" | bc)
    AVG_4B=$(echo "scale=3; $TIME_4B / 3" | bc)
    echo "Total: ${TIME_4B}s (avg: ${AVG_4B}s per query)"
    echo ""

    SPEEDUP=$(echo "scale=2; $TIME_8B / $TIME_4B" | bc)
    echo "üìä Comparison:"
    echo "  8B model: ${TIME_8B}s (${AVG_8B}s avg) - 16GB VRAM, dim=4096, MTEB=70.58"
    echo "  4B model: ${TIME_4B}s (${AVG_4B}s avg) - 8GB VRAM, dim=2560, MTEB=~68.5"
    echo "  4B is ${SPEEDUP}x faster"
    echo ""
    echo "Trade-off: 4B uses half the VRAM and is faster, but slightly lower quality"

# Ingest with 4B model (uses separate database)
ingest-4b file:
    uv run lance-db-example ingest {{file}} --model "Qwen/Qwen3-Embedding-4B" --db lancedb_data_4b

# Search with 4B model (uses separate database)
search-4b query limit="5":
    uv run lance-db-example search "{{query}}" --limit {{limit}} --model "Qwen/Qwen3-Embedding-4B" --db lancedb_data_4b

# Fetch library with 4B model
fetch-4b library:
    #!/usr/bin/env python3
    import subprocess
    import sys

    library = "{{ library }}"

    # Use the fetch-library logic but override the ingest to use 4B model
    print(f"Note: Using Qwen3-4B model (2560-dim, 8GB VRAM)")
    print(f"      Data will be stored in lancedb_data_4b/")
    print()

    # For now, just remind user to use --model flag
    print("To fetch with 4B model:")
    print(f'  1. Download: curl https://context7.com/[library-id]/llms.txt > file.md')
    print(f'  2. Ingest: just ingest-4b file.md')
    sys.exit(0)

# Ingest with 0.6B model (uses separate database)
ingest-0_6b file:
    uv run lance-db-example ingest {{file}} --model "Qwen/Qwen3-Embedding-0.6B" --db lancedb_data_0_6b

# Search with 0.6B model (uses separate database)
search-0_6b query limit="5":
    uv run lance-db-example search "{{query}}" --limit {{limit}} --model "Qwen/Qwen3-Embedding-0.6B" --db lancedb_data_0_6b

# Benchmark with Qwen3-Embedding-0.6B model
benchmark-0_6b:
    #!/usr/bin/env bash
    set -e

    MODEL="Qwen/Qwen3-Embedding-0.6B"
    DB="lancedb_data_0_6b"

    # Check if 0.6B database exists
    if [ ! -d "$DB" ]; then
        echo "‚ö†Ô∏è  0.6B model database not found at $DB"
        echo "Create it first:"
        echo "  just ingest-0_6b tests/data/solid.md"
        exit 1
    fi

    echo "üî¨ Benchmarking Qwen3-Embedding-0.6B (1.2GB VRAM)..."
    echo "Using database: $DB"
    echo ""

    QUERIES=("button component" "authentication" "API routing")

    # GPU benchmark
    echo "‚ö° GPU with 0.6B model..."
    START=$(date +%s.%N)
    for q in "${QUERIES[@]}"; do
        uv run lance-db-example search "$q" --limit 3 --model "$MODEL" --db "$DB" > /dev/null 2>&1
    done
    END=$(date +%s.%N)
    GPU_TOTAL=$(echo "$END - $START" | bc)
    GPU_AVG=$(echo "scale=3; $GPU_TOTAL / 3" | bc)
    echo "Total: ${GPU_TOTAL}s (avg: ${GPU_AVG}s per query)"
    echo ""

    # CPU benchmark
    echo "üêå CPU with 0.6B model..."
    START=$(date +%s.%N)
    for q in "${QUERIES[@]}"; do
        CUDA_VISIBLE_DEVICES='' uv run lance-db-example search "$q" --limit 3 --model "$MODEL" --db "$DB" > /dev/null 2>&1
    done
    END=$(date +%s.%N)
    CPU_TOTAL=$(echo "$END - $START" | bc)
    CPU_AVG=$(echo "scale=3; $CPU_TOTAL / 3" | bc)
    echo "Total: ${CPU_TOTAL}s (avg: ${CPU_AVG}s per query)"
    echo ""

    SPEEDUP=$(echo "scale=2; $CPU_TOTAL / $GPU_TOTAL" | bc)
    echo "üìä Results (Qwen3-0.6B):"
    echo "  GPU: ${GPU_TOTAL}s (${GPU_AVG}s avg)"
    echo "  CPU: ${CPU_TOTAL}s (${CPU_AVG}s avg)"
    echo "  Speedup: ${SPEEDUP}x"
    echo "  VRAM: ~1.2GB"
    echo "  Good for: CPU inference, edge devices, high throughput"

# Compare all three Qwen models (8B, 4B, 0.6B)
compare-all-models:
    #!/usr/bin/env bash
    set -e

    echo "üî¨ Comparing all Qwen3 embedding models..."
    echo ""

    # Create databases if missing
    [ -d "lancedb_data" ] || { echo "Creating 8B database..."; just ingest-sample; }
    [ -d "lancedb_data_4b" ] || { echo "Creating 4B database..."; just ingest-4b tests/data/solid.md; }
    [ -d "lancedb_data_0_6b" ] || { echo "Creating 0.6B database..."; just ingest-0_6b tests/data/solid.md; }

    QUERY="button component"

    # Test 8B
    echo "‚ö° Testing Qwen3-8B..."
    START=$(date +%s.%N)
    for i in {1..3}; do
        uv run lance-db-example search "$QUERY" --limit 3 --model "Qwen/Qwen3-Embedding-8B" --db lancedb_data > /dev/null 2>&1
    done
    END=$(date +%s.%N)
    TIME_8B=$(echo "$END - $START" | bc)

    # Test 4B
    echo "‚ö° Testing Qwen3-4B..."
    START=$(date +%s.%N)
    for i in {1..3}; do
        uv run lance-db-example search "$QUERY" --limit 3 --model "Qwen/Qwen3-Embedding-4B" --db lancedb_data_4b > /dev/null 2>&1
    done
    END=$(date +%s.%N)
    TIME_4B=$(echo "$END - $START" | bc)

    # Test 0.6B
    echo "‚ö° Testing Qwen3-0.6B..."
    START=$(date +%s.%N)
    for i in {1..3}; do
        uv run lance-db-example search "$QUERY" --limit 3 --model "Qwen/Qwen3-Embedding-0.6B" --db lancedb_data_0_6b > /dev/null 2>&1
    done
    END=$(date +%s.%N)
    TIME_0_6B=$(echo "$END - $START" | bc)

    echo ""
    echo "üìä Full Model Comparison:"
    echo "  8B:   ${TIME_8B}s - 16GB VRAM, highest quality (MTEB 70.58)"
    echo "  4B:   ${TIME_4B}s - 8GB VRAM, balanced"
    echo "  0.6B: ${TIME_0_6B}s - 1.2GB VRAM, fastest/smallest"
    echo ""
    echo "Speedup vs 8B:"
    echo "  4B is $(echo "scale=2; $TIME_8B / $TIME_4B" | bc)x faster"
    echo "  0.6B is $(echo "scale=2; $TIME_8B / $TIME_0_6B" | bc)x faster"


# Fetch high-priority libraries (recommended to start with)
fetch-priority:
    #!/usr/bin/env python3
    import subprocess
    import sys

    # High priority libraries based on your current work
    libraries = [
        # New job prep
        "vue",
        "nuxt",

        # Current stack
        "solid-js",
        "@solidjs/start",
        "tailwindcss",
        "daisyui",

        # Databases & Analytics
        "duckdb",
        "better-sqlite3",
        "postgres",

        # Python Backend
        "fastapi",
        "pydantic",
        "typer",

        # Data Science Core
        "pandas",
        "polars",
        "numpy",

        # ML/AI
        "torch",
        "transformers",
        "langchain",

        # Validation
        "zod",

        # Testing
        "vitest",
        "pytest",
    ]

    print(f"üéØ Fetching {len(libraries)} high-priority libraries...")
    print()

    for i, library in enumerate(libraries, 1):
        print(f"[{i}/{len(libraries)}] {library}")
        try:
            subprocess.run(["just", "fetch-library", library], timeout=120)
        except Exception as e:
            print(f"  ‚ö†Ô∏è  Error: {e}")
        print()

    print("‚úÖ Priority libraries complete!")

# Fetch all libraries from the list below
fetch-all:
    #!/usr/bin/env python3
    import subprocess
    import sys

    # Libraries to fetch (organized by category)
    libraries = [
        # Frontend Frameworks
        "solid-js",
        "@solidjs/router",
        "@solidjs/start",
        "@kobalte/core",
        "vue",
        "nuxt",

        # UI & Styling
        "tailwindcss",
        "daisyui",
        "chart.js",

        # Data Tables & Visualization
        "@tanstack/solid-table",
        "@tanstack/solid-virtual",
        "papaparse",
        "d3",

        # Databases & Data Tools
        "duckdb",
        "better-sqlite3",
        "postgres",
        "clickhouse",
        "polars",
        "pandas",

        # Python Backend & API
        "fastapi",
        "typer",
        "pydantic",
        "requests",

        # Rust Backend
        "poem",
        "tokio",
        "serde",

        # Python ML/Data Science
        "torch",
        "numpy",
        "scipy",
        "matplotlib",
        "seaborn",
        "transformers",
        "langchain",
        "onnx",

        # TypeScript/JavaScript ML
        # "langchain",  # Duplicate - will use Python version
        "onnxruntime-web",

        # Validation
        "zod",

        # Testing
        "vitest",
        "@playwright/test",
        "pytest",

        # Authentication
        "auth.js",

        # Real-time
        "ws",
        "socket.io",

        # Runtimes & Tools
        "deno",
        "bun",

        # DevOps & Containers
        "docker",
        "podman",

        # Build Tools
        "vite",

        # Languages
        "typescript",
        "rust",
        "go",
        "python",

        # Monorepo
        "turborepo",

        # Logging
        "pino",
        "structlog",

        # Vector Search
        "lancedb",
        "pgvector",

        # Utilities
        "fuse.js",
        "leaflet",
    ]

    print(f"üìö Fetching {len(libraries)} libraries from Context7...")
    print()

    success = []
    failed = []
    skipped = []

    for i, library in enumerate(libraries, 1):
        print(f"[{i}/{len(libraries)}] Processing: {library}")

        try:
            result = subprocess.run(
                ["just", "fetch-library", library],
                capture_output=True,
                text=True,
                timeout=120  # 2 minute timeout per library
            )

            if result.returncode == 0:
                success.append(library)
                print(f"  ‚úÖ Success")
            else:
                # Check if it's a "not found" error vs actual failure
                if "No libraries found" in result.stdout or "Failed to parse" in result.stdout:
                    skipped.append(library)
                    print(f"  ‚è≠Ô∏è  Not found on Context7")
                else:
                    failed.append(library)
                    print(f"  ‚ùå Failed")
                    print(f"     {result.stdout[:100]}")

        except subprocess.TimeoutExpired:
            failed.append(library)
            print(f"  ‚è±Ô∏è  Timeout (>2min)")
        except Exception as e:
            failed.append(library)
            print(f"  ‚ùå Error: {e}")

        print()

    # Summary
    print("=" * 60)
    print("üìä Summary:")
    print(f"  ‚úÖ Success: {len(success)}")
    print(f"  ‚è≠Ô∏è  Skipped: {len(skipped)}")
    print(f"  ‚ùå Failed: {len(failed)}")
    print()

    if failed:
        print("Failed libraries:")
        for lib in failed:
            print(f"  - {lib}")
        print()

    if skipped:
        print("Skipped (not found on Context7):")
        for lib in skipped:
            print(f"  - {lib}")

    sys.exit(0 if not failed else 1)

# List all documented fetch groups.
fetch-docs-groups:
    #!/usr/bin/env python3
    groups = [
        "frontend",
        "ui",
        "data_viz",
        "db_tools",
        "python_backend",
        "rust_backend",
        "python_ml",
        "js_ml",
        "validation",
        "testing",
        "auth",
        "realtime",
        "runtimes",
        "devops",
        "build_tools",
        "languages",
        "monorepo",
        "logging",
        "vector_search",
        "utilities",
        "all",
    ]
    print("Available groups:")
    for name in groups:
        print(f"  - {name}")

# Fetch a documented group by name. Example: just fetch-docs-group frontend "solidjs routing docs"
fetch-docs-group group query="":
    #!/usr/bin/env python3
    import subprocess
    import sys

    target_group = "{{ group }}"
    query_override = "{{ query }}".strip()

    groups = {
        "frontend": [
            "solid-js", "@solidjs/router", "@solidjs/start", "@kobalte/core",
            "vue", "nuxt",
        ],
        "ui": ["tailwindcss", "daisyui", "chart.js"],
        "data_viz": ["@tanstack/solid-table", "@tanstack/solid-virtual", "papaparse", "d3"],
        "db_tools": ["duckdb", "better-sqlite3", "postgres", "clickhouse", "polars", "pandas"],
        "python_backend": ["fastapi", "typer", "pydantic", "requests"],
        "rust_backend": ["poem", "tokio", "serde"],
        "python_ml": ["torch", "numpy", "scipy", "matplotlib", "seaborn", "transformers", "langchain", "onnx"],
        "js_ml": ["onnxruntime-web"],
        "validation": ["zod"],
        "testing": ["vitest", "@playwright/test", "pytest"],
        "auth": ["auth.js"],
        "realtime": ["ws", "socket.io"],
        "runtimes": ["deno", "bun"],
        "devops": ["docker", "podman"],
        "build_tools": ["vite"],
        "languages": ["typescript", "rust", "go", "python"],
        "monorepo": ["turborepo"],
        "logging": ["pino", "structlog"],
        "vector_search": ["lancedb", "pgvector"],
        "utilities": ["fuse.js", "leaflet"],
    }

    library_queries = {
        "@solidjs/start": "SolidStart framework routing redirect server actions",
        "@solidjs/router": "Solid Router routes navigation params",
        "@kobalte/core": "Kobalte Solid UI component library docs",
        "chart.js": "Chart.js JavaScript charting documentation",
        "postgres": "PostgreSQL official documentation SQL reference",
        "torch": "PyTorch tensors autograd nn module docs",
        "transformers": "Hugging Face Transformers models pipelines tokenizers docs",
        "langchain": "LangChain Python documentation chains agents retrievers",
        "auth.js": "Auth.js authentication providers sessions callbacks docs",
        "ws": "ws Node.js WebSocket library API docs",
        "python": "Python language standard library documentation",
        "go": "Go language official documentation standard library",
        "rust": "Rust language standard library and cargo docs",
        "lancedb": "LanceDB vector database Python docs",
        "pgvector": "pgvector PostgreSQL vector similarity extension docs",
    }

    if target_group == "all":
        seen = set()
        libraries = []
        for libs in groups.values():
            for lib in libs:
                if lib not in seen:
                    seen.add(lib)
                    libraries.append(lib)
    else:
        libraries = groups.get(target_group)
        if libraries is None:
            print(f"Unknown group: {target_group}")
            print("Run `just fetch-docs-groups` to see valid group names.")
            sys.exit(1)

    print(f"Fetching {len(libraries)} libraries from group '{target_group}'...")
    print()

    success = 0
    failed = 0

    for i, library in enumerate(libraries, 1):
        print(f"[{i}/{len(libraries)}] {library}")
        query = query_override or library_queries.get(
            library, f"{library} official documentation API reference"
        )
        result = subprocess.run(["just", "fetch-library", library, query], timeout=180)
        if result.returncode == 0:
            success += 1
        else:
            failed += 1
        print()

    print(f"Done. Success: {success}, Failed: {failed}")
    sys.exit(0 if failed == 0 else 1)

# Fetch every documentation target listed in this justfile's categorized sections.
fetch-all-described query="":
    just fetch-docs-group all "{{query}}"

# Libraries appropriate for Context7 documentation fetching:
#
# === Frontend Frameworks ===
# solid-js
# @solidjs/router
# @solidjs/start
# @kobalte/core
# vue
# nuxt
#
# === UI & Styling ===
# tailwindcss
# daisyui
# chart.js
#
# === Data Tables & Visualization ===
# @tanstack/solid-table
# @tanstack/solid-virtual
# papaparse
# d3
#
# === Databases & Data Tools ===
# duckdb
# better-sqlite3
# postgres (PostgreSQL docs)
# clickhouse
# polars (Python/Rust dataframes)
# pandas
#
# === Python Backend & API ===
# fastapi
# typer
# pydantic
# requests
#
# === Rust Backend ===
# poem
# tokio
# serde
#
# === Python ML/Data Science ===
# torch (PyTorch)
# numpy
# scipy
# matplotlib
# seaborn
# transformers (Hugging Face)
# langchain
# onnx
#
# === TypeScript/JavaScript ML ===
# langchain (JS version)
# onnxruntime-web
#
# === Validation ===
# zod
#
# === Testing ===
# vitest
# @playwright/test
# pytest
#
# === Authentication ===
# auth.js (formerly NextAuth)
#
# === Real-time ===
# ws (WebSocket library)
# socket.io
#
# === Runtimes & Tools ===
# deno
# bun
#
# === DevOps & Containers ===
# docker
# podman
#
# === Build Tools ===
# vite
#
# === Languages ===
# typescript
# rust
# go
# python
#
# === Monorepo (TODO: evaluate) ===
# turborepo
#
# === Logging (TODO: evaluate) ===
# pino (Node.js structured logging)
# structlog (Python structured logging)
#
# === Vector Search ===
# lancedb
# pgvector
#
# === Utilities ===
# fuse.js (fuzzy search)
# leaflet (maps)
